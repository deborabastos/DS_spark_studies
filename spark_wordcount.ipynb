{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/10 22:19:50 WARN Utils: Your hostname, Deboras-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.68.102 instead (on interface en0)\n",
      "22/12/10 22:19:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/10 22:19:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ : 6\n",
      " : 305\n",
      "\"cells\": : 1\n",
      "[ : 7\n",
      "\"cell_type\": : 2\n",
      "\"code\", : 2\n",
      "\"execution_count\": : 2\n",
      "1, : 1\n",
      "\"metadata\": : 2\n",
      "{}, : 2\n",
      "\"outputs\": : 2\n",
      "[], : 1\n",
      "\"source\": : 2\n",
      "\"import : 1\n",
      "sys\\n\", : 1\n",
      "\"from : 2\n",
      "operator : 1\n",
      "import : 2\n",
      "add\\n\", : 1\n",
      "pyspark.sql : 1\n",
      "SparkSession\" : 1\n",
      "] : 4\n",
      "}, : 3\n",
      "2, : 1\n",
      "\"name\": : 3\n",
      "\"stdout\", : 2\n",
      "\"output_type\": : 3\n",
      "\"stream\", : 3\n",
      "\"text\": : 3\n",
      "\"22/12/10 : 3\n",
      "22:19:50 : 3\n",
      "WARN : 3\n",
      "Utils: : 2\n",
      "Your : 1\n",
      "hostname, : 1\n",
      "Deboras-MacBook-Pro.local : 1\n",
      "resolves : 1\n",
      "to : 5\n",
      "a : 1\n",
      "loopback : 1\n",
      "address: : 1\n",
      "127.0.0.1; : 1\n",
      "using : 2\n",
      "192.168.68.102 : 1\n",
      "instead : 1\n",
      "(on : 1\n",
      "interface : 1\n",
      "en0)\\n\", : 1\n",
      "Set : 1\n",
      "SPARK_LOCAL_IP : 1\n",
      "if : 1\n",
      "you : 1\n",
      "need : 1\n",
      "bind : 1\n",
      "another : 1\n",
      "address\\n\" : 1\n",
      "\"stderr\", : 1\n",
      "\"Setting : 1\n",
      "default : 1\n",
      "log : 1\n",
      "level : 2\n",
      "\\\"WARN\\\".\\n\", : 1\n",
      "\"To : 1\n",
      "adjust : 1\n",
      "logging : 1\n",
      "use : 2\n",
      "sc.setLogLevel(newLevel). : 1\n",
      "For : 1\n",
      "SparkR, : 1\n",
      "setLogLevel(newLevel).\\n\" : 1\n",
      "NativeCodeLoader: : 1\n",
      "Unable : 1\n",
      "load : 1\n",
      "native-hadoop : 1\n",
      "library : 1\n",
      "for : 1\n",
      "your : 1\n",
      "platform... : 1\n",
      "builtin-java : 1\n",
      "classes : 1\n",
      "where : 1\n",
      "applicable\\n\" : 1\n",
      "} : 1\n",
      "], : 1\n",
      "\"if : 1\n",
      "__name__ : 1\n",
      "== : 1\n",
      "\\\"__main__\\\":\\n\", : 1\n",
      "\" : 12\n",
      "\\n\", : 3\n",
      "spark : 1\n",
      "= : 4\n",
      "SparkSession : 1\n",
      "\\\\\\n\", : 5\n",
      ".builder : 1\n",
      ".appName(\\\"PythonWordCount\\\") : 1\n",
      ".getOrCreate()\\n\", : 1\n",
      "lines : 1\n",
      "spark.read.text(\\\"./spark_wordcount.ipynb\\\").rdd.map(lambda : 1\n",
      "r: : 1\n",
      "r[0])\\n\", : 1\n",
      "counts : 1\n",
      "lines.flatMap(lambda : 1\n",
      "x: : 2\n",
      "x.split(' : 1\n",
      "')) : 1\n",
      ".map(lambda : 1\n",
      "(x,1)) : 1\n",
      ".reduceByKey(add)\\n\", : 1\n",
      "\"\\n\", : 1\n",
      "output : 1\n",
      "counts.collect()\\n\", : 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"PythonWordCount\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    lines = spark.read.text(\"./spark_wordcount.ipynb\").rdd.map(lambda r: r[0])\n",
    "    counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "                    .map(lambda x: (x,1)) \\\n",
    "                    .reduceByKey(add)\n",
    "    \n",
    "\n",
    "    output = counts.collect()\n",
    "    for word, count in output:\n",
    "        print(\"%s : %i\" % (word, count))\n",
    "    \n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3 (v3.10.3:a342a49189, Mar 16 2022, 09:34:18) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
