{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 02:29:32 WARN Utils: Your hostname, Deboras-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.68.104 instead (on interface en0)\n",
      "23/01/31 02:29:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 02:29:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/01/31 02:29:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Desafio Módulo 2 - ML\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df = spark.read.csv('./stroke_data.csv', header='True', inferSchema='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67135"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quantos registros existem no arquivo?\n",
    "stroke_df.count()\n",
    "# R: 67135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- hypertension: integer (nullable = true)\n",
      " |-- heart_disease: integer (nullable = true)\n",
      " |-- ever_married: string (nullable = true)\n",
      " |-- work_type: string (nullable = true)\n",
      " |-- Residence_type: string (nullable = true)\n",
      " |-- avg_glucose_level: double (nullable = true)\n",
      " |-- bmi: double (nullable = true)\n",
      " |-- smoking_status: string (nullable = true)\n",
      " |-- stroke: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Quantas colunas existem no arquivo? Quantas são numéricas? Ao ler o arquivo com spark.read.csv, habilite inferSchema=True. Use a \n",
    "# função printSchema() da API de Dataframes.\n",
    "stroke_df.printSchema()\n",
    "# R: 12 e 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+------------+-------------+------------+-------------+--------------+-----------------+-----+---------------+------+\n",
      "|  0|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level|  bmi| smoking_status|stroke|\n",
      "+---+------+----+------------+-------------+------------+-------------+--------------+-----------------+-----+---------------+------+\n",
      "|  1|Female|18.0|           0|            0|          No|      Private|         Urban|            94.19|12.12|         smokes|     1|\n",
      "|  2|  Male|58.0|           1|            0|         Yes|      Private|         Rural|           154.24| 33.7|   never_smoked|     0|\n",
      "|  3|Female|36.0|           0|            0|         Yes|     Govt_job|         Urban|            72.63| 24.7|         smokes|     0|\n",
      "|  4|Female|62.0|           0|            0|         Yes|Self-employed|         Rural|            85.52| 31.2|formerly smoked|     0|\n",
      "|  5|Female|82.0|           0|            0|         Yes|      Private|         Rural|            59.32| 33.2|         smokes|     1|\n",
      "|  6|Female|82.0|           0|            0|          No|     Govt_job|         Urban|            234.5| 24.0|formerly smoked|     0|\n",
      "|  7|Female|33.0|           0|            0|         Yes|Self-employed|         Urban|           193.42| 29.9|         smokes|     0|\n",
      "|  8|Female|37.0|           0|            0|         Yes|      Private|         Rural|            156.7| 36.9|         smokes|     1|\n",
      "|  9|Female|41.0|           0|            0|         Yes|     Govt_job|         Rural|            64.06| 33.8|         smokes|     1|\n",
      "| 10|Female|70.0|           0|            0|         Yes|Self-employed|         Rural|            76.34| 24.4|formerly smoked|     1|\n",
      "| 11|Female|25.0|           0|            0|          No|      Private|         Urban|            91.15| 28.7|         smokes|     1|\n",
      "| 12|Female|43.0|           1|            0|          No|Self-employed|         Rural|            60.12| 34.2|formerly smoked|     0|\n",
      "| 13|  Male|72.0|           0|            1|         Yes|      Private|         Rural|           235.22| 40.3|formerly smoked|     1|\n",
      "| 14|Female|20.0|           0|            0|          No|      Private|         Rural|           106.47| 33.7|         smokes|     1|\n",
      "| 15|  Male|20.0|           0|            0|          No|      Private|         Urban|           104.78| 20.3|         smokes|     1|\n",
      "| 16|  Male|41.0|           0|            0|         Yes|Self-employed|         Urban|            159.3| 34.6|         smokes|     1|\n",
      "| 17|Female|23.0|           0|            0|          No|      Private|         Urban|           116.95| 23.8|         smokes|     1|\n",
      "| 18|  Male|22.0|           0|            0|          No|Self-employed|         Rural|            72.05| 31.9|         smokes|     1|\n",
      "| 19|  Male|69.0|           0|            0|         Yes|      Private|         Rural|            64.06| 35.1|formerly smoked|     0|\n",
      "| 20|Female|44.0|           0|            0|         Yes|Self-employed|         Rural|           135.03| 36.1|         smokes|     1|\n",
      "+---+------+----+------------+-------------+------------+-------------+--------------+-----------------+-----+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stroke_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|stroke|count|\n",
      "+------+-----+\n",
      "|     1|40287|\n",
      "|     0|26848|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# No conjunto de dados, quantos pacientes sofreram e não sofreram derrame (stroke), respectivamente?\n",
    "stroke_df.groupBy('stroke').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|    work_type|total_stroke|\n",
      "+-------------+------------+\n",
      "| Never_worked|          85|\n",
      "|Self-employed|       10807|\n",
      "|      Private|       23711|\n",
      "|     children|         520|\n",
      "|     Govt_job|        5164|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#A partir do dataframe, crie uma tabela temporária usando df.createOrReplaceTempView('table') e a seguir use spark.sql para escrever \n",
    "# uma consulta SQL que obtenha quantos pacientes tiveram derrame por tipo de trabalho (work_type). Quantos pacientes sofreram derrame \n",
    "# e trabalhavam respectivamente, no setor privado, de forma independente, no governo e quantas são crianças?\n",
    "\n",
    "stroke_df.createOrReplaceTempView('temp_stroke')\n",
    "df1 = spark.sql(\"SELECT work_type, COUNT(stroke) AS total_stroke FROM temp_stroke WHERE stroke = 1 GROUP BY work_type\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|gender|number|\n",
      "+------+------+\n",
      "|Female| 39530|\n",
      "| Other|    11|\n",
      "|  Male| 27594|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Escreva uma consulta com spark.sql para determinar a proporção, por gênero, de participantes do estudo. A maioria dos participantes é:\n",
    "stroke_df.createOrReplaceTempView('temp_stroke2')\n",
    "df_gender = spark.sql(\"SELECT gender, COUNT(0) AS number FROM temp_stroke2 GROUP By gender\")\n",
    "df_gender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+------------+------+-----+----------+\n",
      "|hypertension|stroke|total|percentage|\n",
      "+------------+------+-----+----------+\n",
      "|           1|     0| 2200|     19.97|\n",
      "|           1|     1| 8817|     80.03|\n",
      "+------------+------+-----+----------+\n",
      "\n",
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 02:29:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+------------+------+-----+----------+\n",
      "|hypertension|stroke|total|percentage|\n",
      "+------------+------+-----+----------+\n",
      "|           0|     0|24648|     43.92|\n",
      "|           0|     1|31470|     56.08|\n",
      "+------------+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Escreva uma consulta com spark.sql para determinar quem tem mais probabilidade de sofrer derrame: hipertensos ou não-hipertensos. \n",
    "# Você pode escrever uma consulta para cada grupo. A partir das probabilidades que você obteve, você conclui que:\n",
    "stroke_df.createOrReplaceTempView('temp_stroke3')\n",
    "# df_hipert_stroke = spark.sql(\"SELECT hypertension, stroke, COUNT(stroke) AS total_hyp FROM temp_stroke3 GROUP BY hypertension, stroke\")\n",
    "# df_hipert_stroke.show()\n",
    "\n",
    "df1 = spark.sql(\"SELECT hypertension, stroke, COUNT(stroke) AS total, ROUND(COUNT(stroke) * 100 / sum(COUNT(stroke)) OVER (),2) AS percentage FROM temp_stroke3 WHERE hypertension = 1 GROUP BY hypertension, stroke\")\n",
    "df1.show()\n",
    "\n",
    "# df1 = spark.sql(\"SELECT hypertension, stroke, COUNT(stroke) AS total FROM temp_stroke3 GROUP BY hypertension, stroke\")\n",
    "\n",
    "\n",
    "\n",
    "df2 = spark.sql(\"SELECT hypertension, stroke, COUNT(stroke) AS total, ROUND(COUNT(stroke) * 100 / sum(COUNT(stroke)) OVER (),2) AS percentage FROM temp_stroke3 WHERE hypertension = 0 GROUP BY hypertension, stroke\")\n",
    "df2.show()\n",
    "\n",
    "# A hipertensão, neste conjunto de dados, aumenta a probabilidade de derrame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "| age|count(stroke)|\n",
      "+----+-------------+\n",
      "|79.0|         3258|\n",
      "|78.0|         2672|\n",
      "|80.0|         2141|\n",
      "|81.0|         2005|\n",
      "|82.0|         1657|\n",
      "|63.0|         1294|\n",
      "|66.0|         1195|\n",
      "|77.0|         1190|\n",
      "|74.0|         1184|\n",
      "|57.0|         1160|\n",
      "|70.0|         1151|\n",
      "|76.0|         1088|\n",
      "|67.0|         1070|\n",
      "|51.0|         1067|\n",
      "|65.0|         1046|\n",
      "|75.0|         1015|\n",
      "|52.0|         1003|\n",
      "|58.0|          999|\n",
      "|59.0|          994|\n",
      "|61.0|          988|\n",
      "+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Escreva uma consulta com spark.sql que determine o número de pessoas que sofreram derrame por idade. \n",
    "# Com qual idade o maior número de pessoas do conjunto de dados sofreu derrame?\n",
    "stroke_df.createOrReplaceTempView('temp_stroke4')\n",
    "\n",
    "age_df = spark.sql(\"SELECT age, COUNT(stroke) FROM temp_stroke4 GROUP BY age ORDER BY count(stroke) DESC\")\n",
    "age_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28938"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Usando a API de dataframes, determine quantas pessoas sofreram derrames após os 50 anos.\n",
    "stroke_df.filter((stroke_df.age > 50) & (stroke_df.stroke == 1)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+\n",
      "|stroke|avg(avg_glucose_level)|\n",
      "+------+----------------------+\n",
      "|     1|    119.95307046938272|\n",
      "|     0|    103.60273130214506|\n",
      "+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Usando spark.sql, determine qual o nível médio de glicose para pessoas que, respectivamente, sofreram e não sofreram derrame.\n",
    "stroke_df.createOrReplaceTempView('temp_stroke5')\n",
    "glic_df = spark.sql(\"SELECT stroke, AVG(avg_glucose_level) FROM temp_stroke5 GROUP BY stroke\")\n",
    "glic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|stroke|          avg(bmi)|\n",
      "+------+------------------+\n",
      "|     1|29.942490629729495|\n",
      "|     0|27.989678933253657|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Qual é o BMI (IMC = índice de massa corpórea) médio de quem sofreu e não sofreu derrame?\n",
    "stroke_df.createOrReplaceTempView('temp_stroke6')\n",
    "imc_df = spark.sql(\"SELECT stroke, AVG(bmi) FROM temp_stroke6 GROUP BY stroke\")\n",
    "imc_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando modelo DE Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie um modelo de árvore de decisão que prevê a chance de derrame (stroke) a partir das variáveis contínuas/categóricas: idade, \n",
    "# BMI, hipertensão, doença do coração, nível médio de glicose. Use o conteúdo da segunda aula interativa para criar e avaliar o modelo.\n",
    "\n",
    "# Primeiro é necessário tornar o dado legível para ML. Dafaframe, tabela não são a melhor opção. Melhor usar vetores.\n",
    "# Vector Assembler transforma os dados do DF em vetores legíveis para ML\n",
    "\n",
    "# Montar o dado para o algorítmo entender\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['age', 'bmi', 'hypertension', 'heart_disease', 'avg_glucose_level'], outputCol='features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar classificadores ????\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(labelCol='stroke', featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar pipeline\n",
    "# Abstração que permite compor as etapas do preocesso de ML\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir dados em treino e teste\n",
    "\n",
    "train_data, test_data = stroke_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função .fit() roda o pipeline intenro do modelo\n",
    "predictStrokeModel = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------------+-------------+----+-----------------+---------------+----------+------+\n",
      "|gender| age|hypertension|heart_disease| bmi|avg_glucose_level|  rawPrediction|prediction|stroke|\n",
      "+------+----+------------+-------------+----+-----------------+---------------+----------+------+\n",
      "|Female|62.0|           0|            0|31.2|            85.52|[2184.0,3394.0]|       1.0|     0|\n",
      "|Female|82.0|           0|            0|24.0|            234.5|[1181.0,7813.0]|       1.0|     0|\n",
      "|Female|70.0|           0|            0|24.4|            76.34|[2184.0,3394.0]|       1.0|     1|\n",
      "|Female|43.0|           1|            0|34.2|            60.12|   [93.0,133.0]|       1.0|     0|\n",
      "|Female|23.0|           0|            0|23.8|           116.95|[4614.0,4064.0]|       0.0|     1|\n",
      "+------+----+------------+-------------+----+-----------------+---------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rodar nos dados de teste\n",
    "\n",
    "predictions = predictStrokeModel.transform(test_data)\n",
    "predictions.select('gender', 'age', 'hypertension', 'heart_disease', 'bmi', 'avg_glucose_level', 'rawPrediction', 'prediction', 'stroke').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6943261709080107"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Verificar acurácia do modelo\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='stroke', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "accuracy\n",
    "\n",
    "# Resposta: Menor que 75% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " 'gender',\n",
       " 'age',\n",
       " 'hypertension',\n",
       " 'heart_disease',\n",
       " 'ever_married',\n",
       " 'work_type',\n",
       " 'Residence_type',\n",
       " 'avg_glucose_level',\n",
       " 'bmi',\n",
       " 'smoking_status',\n",
       " 'stroke']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stroke_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicione ao modelo as variáveis categóricas: gênero e status de fumante. Use o conteúdo da aula interativa para lidar \n",
    "# com as variáveis categóricas.  A acurácia (qualidade) do modelo aumentou para:\n",
    "\n",
    "# É necessário categorizar a variável GENDER e smoking_status\n",
    "#Data type string of column gender is not supported.\n",
    "#Data type string of column smoking_status is not supported.\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "gender_indexer = StringIndexer(inputCol='gender', outputCol='genderIndexer')\n",
    "gender_encoder = OneHotEncoder(inputCol='genderIndexer', outputCol='genderVector')\n",
    "\n",
    "\n",
    "smoking_status_indexer = StringIndexer(inputCol='smoking_status', outputCol='smoking_statusIndexer')\n",
    "smoking_status_encoder = OneHotEncoder(inputCol='smoking_statusIndexer', outputCol='smoking_statusVector')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8371571072319202"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['genderVector', 'age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi', 'smoking_statusVector'], outputCol='features')\n",
    "classifier = DecisionTreeClassifier(labelCol='stroke', featuresCol='features')\n",
    "pipeline = Pipeline(stages=[gender_indexer,gender_encoder,smoking_status_indexer, smoking_status_encoder, assembler, classifier])\n",
    "train_data, test_data = stroke_df.randomSplit([0.7, 0.3])\n",
    "predictStrokeModel = pipeline.fit(train_data)\n",
    "predictions = predictStrokeModel.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='stroke', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('genderVector', 0.00023438035326561605),\n",
       " ('age', 0.1724908757031725),\n",
       " ('hypertension', 0.006007246339692424),\n",
       " ('heart_disease', 0.0011331132084078143),\n",
       " ('avg_glucose_level', 0.4834705720873366),\n",
       " ('bmi', 0.3366638123081251)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Qual dessas variáveis é mais importante no modelo de árvore de decisão que você construiu na questão (12)?\n",
    "decisionTreeModel = predictStrokeModel.stages[5]\n",
    "decisionTreeModel\n",
    "\n",
    "list(zip(assembler.getInputCols(), decisionTreeModel.featureImportances.values))\n",
    "#R: smoking_statusVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Qual a profundidade da árvore de decisão da questão (12)?\n",
    "decisionTreeModel.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DecisionTreeClassificationModel: uid=DecisionTreeClassifier_140037484ae3, depth=5, numNodes=25, numClasses=2, numFeatures=9\\n  If (feature 7 in {0.0})\\n   If (feature 8 in {0.0})\\n    Predict: 0.0\\n   Else (feature 8 not in {0.0})\\n    If (feature 2 <= 56.5)\\n     Predict: 0.0\\n    Else (feature 2 > 56.5)\\n     If (feature 2 <= 73.5)\\n      If (feature 5 <= 73.535)\\n       Predict: 0.0\\n      Else (feature 5 > 73.535)\\n       Predict: 1.0\\n     Else (feature 2 > 73.5)\\n      Predict: 1.0\\n  Else (feature 7 not in {0.0})\\n   If (feature 2 <= 66.5)\\n    If (feature 2 <= 18.5)\\n     If (feature 2 <= 13.5)\\n      If (feature 5 <= 199.485)\\n       Predict: 1.0\\n      Else (feature 5 > 199.485)\\n       Predict: 0.0\\n     Else (feature 2 > 13.5)\\n      Predict: 1.0\\n    Else (feature 2 > 18.5)\\n     Predict: 1.0\\n   Else (feature 2 > 66.5)\\n    If (feature 2 <= 73.5)\\n     If (feature 6 <= 20.25)\\n      If (feature 0 in {0.0})\\n       Predict: 0.0\\n      Else (feature 0 not in {0.0})\\n       Predict: 1.0\\n     Else (feature 6 > 20.25)\\n      Predict: 1.0\\n    Else (feature 2 > 73.5)\\n     Predict: 1.0\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver a árvore:\n",
    "decisionTreeModel.toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['genderVector',\n",
       " 'age',\n",
       " 'hypertension',\n",
       " 'heart_disease',\n",
       " 'avg_glucose_level',\n",
       " 'bmi',\n",
       " 'smoking_statusVector']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler.getInputCols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quantos nodos a árvore de decisão possui?\n",
    "decisionTreeModel.numNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisionTreeModel.numFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Salvar modelo (serializar) - persistência do modelo\n",
    "basePath = \"./\"\n",
    "decisionTreeModel.write().overwrite().save(basePath + \"/ML_model_stroke\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
