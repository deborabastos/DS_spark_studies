{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/06 22:37:19 WARN Utils: Your hostname, Deboras-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.68.104 instead (on interface en0)\n",
      "23/01/06 22:37:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/06 22:37:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Estabilish connection and load data into memory\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "        .appName(\"Map\")\n",
    "        .getOrCreate())\n",
    "\n",
    "# Get data\n",
    "data = spark.sparkContext.textFile(\"/opt/homebrew/Cellar/apache-spark/3.3.1/README.md\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('', 0)\n",
      "('```python', 9)\n",
      "('>>> spark.range(1000 * 1000 * 1000).count()', 43)\n",
      "('```', 3)\n",
      "('', 0)('# Apache Spark', 14)\n",
      "\n",
      "('', 0)('## Example Programs', 19)\n",
      "('', 0)\n",
      "('Spark also comes with several sample programs in the `examples` directory.', 74)\n",
      "('To run one of them, use `./bin/run-example <class> [params]`. For example:', 74)\n",
      "('', 0)\n",
      "('```bash', 7)\n",
      "('./bin/run-example SparkPi', 25)\n",
      "('```', 3)\n",
      "('', 0)\n",
      "('will run the Pi example locally.', 32)\n",
      "('', 0)\n",
      "('You can set the MASTER environment variable when running examples to submit', 75)\n",
      "\n",
      "('examples to a cluster. This can be a mesos:// or spark:// URL,', 62)\n",
      "('\"yarn\" to run on YARN, and \"local\" to run', 41)\n",
      "('locally with one thread, or \"local[N]\" to run locally with N threads. You', 73)\n",
      "('can also use an abbreviated class name if the class is in the `examples`', 72)\n",
      "('Spark is a unified analytics engine for large-scale data processing. It provides', 80)('package. For instance:', 22)\n",
      "\n",
      "('', 0)('high-level APIs in Scala, Java, Python, and R, and an optimized engine that', 75)\n",
      "\n",
      "('```bash', 7)\n",
      "('MASTER=spark://host:7077 ./bin/run-example SparkPi', 50)\n",
      "('```', 3)\n",
      "('', 0)\n",
      "('Many of the example programs print usage help if no params are given.', 69)\n",
      "('supports general computation graphs for data analysis. It also supports a', 73)('', 0)\n",
      "\n",
      "('## Running Tests', 16)\n",
      "('', 0)\n",
      "('Testing first requires [building Spark](#building-spark). Once Spark is built, tests', 84)\n",
      "('can be run using:', 17)\n",
      "('rich set of higher-level tools including Spark SQL for SQL and DataFrames,', 74)('', 0)\n",
      "\n",
      "('```bash', 7)('pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing,', 98)\n",
      "('and Structured Streaming for stream processing.', 47)\n",
      "('', 0)\n",
      "('<https://spark.apache.org/>', 27)\n",
      "('', 0)\n",
      "\n",
      "('./dev/run-tests', 15)\n",
      "('```', 3)\n",
      "('', 0)\n",
      "('Please see the guidance on how to', 33)\n",
      "('[![GitHub Action Build](https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master&event=push)](https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster+event%3Apush)', 234)('[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).', 110)\n",
      "\n",
      "('', 0)\n",
      "('[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)', 189)('There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md', 105)\n",
      "\n",
      "('', 0)('[![PySpark Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)', 123)\n",
      "\n",
      "('## A Note About Hadoop Versions', 31)('', 0)\n",
      "('', 0)\n",
      "('## Online Documentation', 23)\n",
      "('', 0)\n",
      "('You can find the latest Spark documentation, including a programming', 68)\n",
      "\n",
      "('guide, on the [project web page](https://spark.apache.org/documentation.html).', 78)\n",
      "('', 0)('This README file only contains basic setup instructions.', 56)\n",
      "('Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported', 77)\n",
      "('storage systems. Because the protocols have changed in different versions of', 76)\n",
      "('Hadoop, you must build Spark against the same version that your cluster runs.', 77)\n",
      "\n",
      "('', 0)\n",
      "('', 0)('## Building Spark', 17)\n",
      "('', 0)\n",
      "('Spark is built using [Apache Maven](https://maven.apache.org/).', 63)\n",
      "('To build Spark and its example programs, run:', 45)\n",
      "('', 0)\n",
      "('```bash', 7)\n",
      "('./build/mvn -DskipTests clean package', 37)\n",
      "('```', 3)\n",
      "('', 0)\n",
      "('(You do not need to do this if you downloaded a pre-built package.)', 67)\n",
      "('', 0)\n",
      "('More detailed documentation is available from the project site, at', 66)\n",
      "('[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).', 77)\n",
      "('', 0)\n",
      "('For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).', 157)\n",
      "('', 0)\n",
      "\n",
      "('Please refer to the build documentation at', 42)\n",
      "('[\"Specifying the Hadoop Version and Enabling YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)', 157)\n",
      "('for detailed guidance on building for a particular distribution of Hadoop, including', 84)\n",
      "('building for particular Hive and Hive Thriftserver distributions.', 65)\n",
      "('', 0)\n",
      "('## Configuration', 16)\n",
      "('', 0)\n",
      "('Please refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)', 98)\n",
      "('in the online documentation for an overview on how to configure Spark.', 70)\n",
      "('', 0)\n",
      "('## Contributing', 15)\n",
      "('', 0)\n",
      "('Please review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)', 91)\n",
      "('for information on how to get started contributing to the project.', 66)\n",
      "('## Interactive Scala Shell', 26)\n",
      "('', 0)\n",
      "('The easiest way to start using Spark is through the Scala shell:', 64)\n",
      "('', 0)\n",
      "('```bash', 7)\n",
      "('./bin/spark-shell', 17)\n",
      "('```', 3)\n",
      "('', 0)\n",
      "('Try the following command, which should return 1,000,000,000:', 61)\n",
      "('', 0)\n",
      "('```scala', 8)\n",
      "('scala> spark.range(1000 * 1000 * 1000).count()', 46)\n",
      "('```', 3)\n",
      "('', 0)\n",
      "('## Interactive Python Shell', 27)\n",
      "('', 0)\n",
      "('Alternatively, if you prefer Python, you can use the Python shell:', 66)\n",
      "('', 0)\n",
      "('```bash', 7)\n",
      "('./bin/pyspark', 13)\n",
      "('```', 3)\n",
      "('', 0)\n",
      "('And run the following command, which should also return 1,000,000,000:', 70)\n"
     ]
    }
   ],
   "source": [
    "# Use map()\n",
    "#For every line in the doc, create a tuple containing its content and lenght (content, lenght)\n",
    "mapFile = data.map(lambda line : (line, len(line)))\n",
    "\n",
    "# Print tuples\n",
    "# foreach() is a action that starts the execution\n",
    "mapFile.foreach(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#\n",
      "Apache\n",
      "Spark\n",
      "Spark\n",
      "is\n",
      "a\n",
      "unified\n",
      "analytics\n",
      "engine\n",
      "for\n",
      "large-scale\n",
      "data\n",
      "processing.\n",
      "It\n",
      "provides\n",
      "high-level\n",
      "APIs\n",
      "in\n",
      "Scala,\n",
      "Java,\n",
      "Python,\n",
      "and\n",
      "R,\n",
      "and\n",
      "an\n",
      "optimized\n",
      "engine\n",
      "that\n",
      "supports\n",
      "general\n",
      "computation\n",
      "graphs\n",
      "for\n",
      "data\n",
      "analysis.\n",
      "It\n",
      "also\n",
      "supports\n",
      "a\n",
      "rich\n",
      "set\n",
      "of\n",
      "higher-level\n",
      "tools\n",
      "including\n",
      "Spark\n",
      "SQL\n",
      "for\n",
      "SQL\n",
      "and\n",
      "DataFrames,\n",
      "pandas\n",
      "API\n",
      "on\n",
      "Spark\n",
      "for\n",
      "pandas\n",
      "workloads,\n",
      "MLlib\n",
      "for\n",
      "machine\n",
      "learning,\n",
      "GraphX\n",
      "for\n",
      "graph\n",
      "processing,\n",
      "and\n",
      "Structured\n",
      "Streaming\n",
      "for\n",
      "stream\n",
      "processing.\n",
      "<https://spark.apache.org/>\n",
      "[![GitHub\n",
      "Action\n",
      "Build](https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master&event=push)](https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster+event%3Apush)\n",
      "[![AppVeyor\n",
      "Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)\n",
      "[![PySpark\n",
      "Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)\n",
      "##\n",
      "Online\n",
      "Documentation\n",
      "You\n",
      "can\n",
      "find\n",
      "the\n",
      "latest\n",
      "Spark\n",
      "documentation,\n",
      "including\n",
      "a\n",
      "programming\n",
      "guide,\n",
      "on\n",
      "the\n",
      "[project\n",
      "web\n",
      "page](https://spark.apache.org/documentation.html).\n",
      "This\n",
      "README\n",
      "file\n",
      "only\n",
      "contains\n",
      "basic\n",
      "setup\n",
      "instructions.\n",
      "##\n",
      "Building\n",
      "Spark\n",
      "Spark\n",
      "is\n",
      "built\n",
      "using\n",
      "[Apache\n",
      "Maven](https://maven.apache.org/).\n",
      "To\n",
      "build\n",
      "Spark\n",
      "and\n",
      "its\n",
      "example\n",
      "programs,\n",
      "run:\n",
      "```bash\n",
      "./build/mvn\n",
      "-DskipTests\n",
      "clean\n",
      "package\n",
      "```\n",
      "(You\n",
      "do\n",
      "not\n",
      "need\n",
      "to\n",
      "do\n",
      "this\n",
      "if\n",
      "you\n",
      "downloaded\n",
      "a\n",
      "pre-built\n",
      "package.)\n",
      "More\n",
      "detailed\n",
      "documentation\n",
      "is\n",
      "available\n",
      "from\n",
      "the\n",
      "project\n",
      "site,\n",
      "at\n",
      "[\"Building\n",
      "Spark\"](https://spark.apache.org/docs/latest/building-spark.html).\n",
      "For\n",
      "general\n",
      "development\n",
      "tips,\n",
      "including\n",
      "info\n",
      "on\n",
      "developing\n",
      "Spark\n",
      "using\n",
      "an\n",
      "IDE,\n",
      "see\n",
      "[\"Useful\n",
      "Developer\n",
      "Tools\"](https://spark.apache.org/developer-tools.html).\n",
      "##\n",
      "Interactive\n",
      "Scala\n",
      "Shell\n",
      "```python\n",
      ">>>\n",
      "spark.range(1000\n",
      "The\n",
      "easiest\n",
      "way\n",
      "to\n",
      "start\n",
      "using\n",
      "Spark\n",
      "is\n",
      "through\n",
      "the\n",
      "Scala\n",
      "shell:\n",
      "*\n",
      "1000\n",
      "*\n",
      "1000).count()\n",
      "```bash\n",
      "./bin/spark-shell\n",
      "``````\n",
      "##\n",
      "Example\n",
      "Programs\n",
      "Spark\n",
      "also\n",
      "comes\n",
      "with\n",
      "\n",
      "several\n",
      "sample\n",
      "programs\n",
      "in\n",
      "the\n",
      "`examples`\n",
      "directory.\n",
      "To\n",
      "run\n",
      "one\n",
      "ofTry\n",
      "\n",
      "them,\n",
      "use\n",
      "`./bin/run-example\n",
      "<class>\n",
      "[params]`.\n",
      "For\n",
      "example:\n",
      "the```bash\n",
      "./bin/run-example\n",
      "SparkPi\n",
      "```\n",
      "\n",
      "willfollowing\n",
      "\n",
      "runcommand,\n",
      "\n",
      "which\n",
      "should\n",
      "return\n",
      "1,000,000,000:\n",
      "the\n",
      "Pi\n",
      "example\n",
      "locally.\n",
      "```scala\n",
      "Youscala>\n",
      "\n",
      "canspark.range(1000\n",
      "\n",
      "set*\n",
      "1000\n",
      "*\n",
      "1000).count()\n",
      "```\n",
      "##\n",
      "Interactive\n",
      "\n",
      "Python\n",
      "Shell\n",
      "the\n",
      "MASTER\n",
      "Alternatively,environment\n",
      "if\n",
      "you\n",
      "variable\n",
      "\n",
      "when\n",
      "runningprefer\n",
      "\n",
      "examplesPython,\n",
      "\n",
      "toyou\n",
      "\n",
      "submitcan\n",
      "\n",
      "use\n",
      "examples\n",
      "the\n",
      "Python\n",
      "shell:\n",
      "to\n",
      "a```bash\n",
      "./bin/pyspark\n",
      "```\n",
      "\n",
      "cluster.\n",
      "And\n",
      "run\n",
      "Thisthe\n",
      "\n",
      "canfollowing\n",
      "\n",
      "becommand,\n",
      "\n",
      "awhich\n",
      "\n",
      "mesos://should\n",
      "\n",
      "or\n",
      "spark://also\n",
      "return\n",
      "1,000,000,000:\n",
      "\n",
      "URL,\n",
      "\"yarn\"\n",
      "to\n",
      "run\n",
      "on\n",
      "YARN,\n",
      "and\n",
      "\"local\"\n",
      "to\n",
      "run\n",
      "locally\n",
      "with\n",
      "one\n",
      "thread,\n",
      "or\n",
      "\"local[N]\"\n",
      "to\n",
      "run\n",
      "locally\n",
      "with\n",
      "N\n",
      "threads.\n",
      "You\n",
      "can\n",
      "also\n",
      "use\n",
      "an\n",
      "abbreviated\n",
      "class\n",
      "name\n",
      "if\n",
      "the\n",
      "class\n",
      "is\n",
      "in\n",
      "the\n",
      "`examples`\n",
      "package.\n",
      "For\n",
      "instance:\n",
      "```bash\n",
      "MASTER=spark://host:7077\n",
      "./bin/run-example\n",
      "SparkPi\n",
      "```\n",
      "Many\n",
      "of\n",
      "the\n",
      "example\n",
      "programs\n",
      "print\n",
      "usage\n",
      "help\n",
      "if\n",
      "no\n",
      "params\n",
      "are\n",
      "given.\n",
      "##\n",
      "Running\n",
      "Tests\n",
      "Testing\n",
      "first\n",
      "requires\n",
      "[building\n",
      "Spark](#building-spark).\n",
      "Once\n",
      "Spark\n",
      "is\n",
      "built,\n",
      "tests\n",
      "can\n",
      "be\n",
      "run\n",
      "using:\n",
      "```bash\n",
      "./dev/run-tests\n",
      "```\n",
      "Please\n",
      "see\n",
      "the\n",
      "guidance\n",
      "on\n",
      "how\n",
      "to\n",
      "[run\n",
      "tests\n",
      "for\n",
      "a\n",
      "module,\n",
      "or\n",
      "individual\n",
      "tests](https://spark.apache.org/developer-tools.html#individual-tests).\n",
      "There\n",
      "is\n",
      "also\n",
      "a\n",
      "Kubernetes\n",
      "integration\n",
      "test,\n",
      "see\n",
      "resource-managers/kubernetes/integration-tests/README.md\n",
      "##\n",
      "A\n",
      "Note\n",
      "About\n",
      "Hadoop\n",
      "Versions\n",
      "Spark\n",
      "uses\n",
      "the\n",
      "Hadoop\n",
      "core\n",
      "library\n",
      "to\n",
      "talk\n",
      "to\n",
      "HDFS\n",
      "and\n",
      "other\n",
      "Hadoop-supported\n",
      "storage\n",
      "systems.\n",
      "Because\n",
      "the\n",
      "protocols\n",
      "have\n",
      "changed\n",
      "in\n",
      "different\n",
      "versions\n",
      "of\n",
      "Hadoop,\n",
      "you\n",
      "must\n",
      "build\n",
      "Spark\n",
      "against\n",
      "the\n",
      "same\n",
      "version\n",
      "that\n",
      "your\n",
      "cluster\n",
      "runs.\n",
      "Please\n",
      "refer\n",
      "to\n",
      "the\n",
      "build\n",
      "documentation\n",
      "at\n",
      "[\"Specifying\n",
      "the\n",
      "Hadoop\n",
      "Version\n",
      "and\n",
      "Enabling\n",
      "YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)\n",
      "for\n",
      "detailed\n",
      "guidance\n",
      "on\n",
      "building\n",
      "for\n",
      "a\n",
      "particular\n",
      "distribution\n",
      "of\n",
      "Hadoop,\n",
      "including\n",
      "building\n",
      "for\n",
      "particular\n",
      "Hive\n",
      "and\n",
      "Hive\n",
      "Thriftserver\n",
      "distributions.\n",
      "##\n",
      "Configuration\n",
      "Please\n",
      "refer\n",
      "to\n",
      "the\n",
      "[Configuration\n",
      "Guide](https://spark.apache.org/docs/latest/configuration.html)\n",
      "in\n",
      "the\n",
      "online\n",
      "documentation\n",
      "for\n",
      "an\n",
      "overview\n",
      "on\n",
      "how\n",
      "to\n",
      "configure\n",
      "Spark.\n",
      "##\n",
      "Contributing\n",
      "Please\n",
      "review\n",
      "the\n",
      "[Contribution\n",
      "to\n",
      "Spark\n",
      "guide](https://spark.apache.org/contributing.html)\n",
      "for\n",
      "information\n",
      "on\n",
      "how\n",
      "to\n",
      "get\n",
      "started\n",
      "contributing\n",
      "to\n",
      "the\n",
      "project.\n"
     ]
    }
   ],
   "source": [
    "# Use flatMap()\n",
    "# Split line into words\n",
    "flatFile = data.flatMap(lambda line : line.split())\n",
    "\n",
    "# Print words\n",
    "flatFile.foreach(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "also\n",
      "a\n",
      "a\n",
      "and\n",
      "also\n",
      "an\n",
      "abbreviated\n",
      "are\n",
      "a\n",
      "also\n",
      "a\n",
      "and\n",
      "against\n",
      "at\n",
      "and\n",
      "a\n",
      "and\n",
      "an\n",
      "a\n",
      "analytics\n",
      "and\n",
      "and\n",
      "an\n",
      "analysis.\n",
      "also\n",
      "a\n",
      "and\n",
      "and\n",
      "a\n",
      "and\n",
      "a\n",
      "available\n",
      "at\n",
      "an\n",
      "also\n"
     ]
    }
   ],
   "source": [
    "# Use filter()\n",
    "# Only words starts with \"a\"\n",
    "filterFile = flatFile.filter(lambda word : word.startswith(\"a\"))\n",
    "\n",
    "# Print words\n",
    "filterFile.foreach(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('três', 1)\n",
      "('um', 2)\n",
      "('dois', 2)\n"
     ]
    }
   ],
   "source": [
    "list = [\"um\", \"um\", \"dois\", \"dois\", \"três\"]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(list) #transform list in RDD\n",
    "\n",
    "# map list into tuple and use reduceByKey to count frequency of every word\n",
    "rdd2 = rdd.map(lambda x: (x, 1)).reduceByKey(lambda a,b : a+b)\n",
    "\n",
    "rdd2.foreach(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('dois', 2)\n",
      "('três', 1)\n",
      "('um', 2)\n"
     ]
    }
   ],
   "source": [
    "# map list into tuple, use reduceByKey to count frequency of every word and sort by key\n",
    "rdd2 = rdd.map(lambda x: (x, 1)).reduceByKey(lambda a,b : a+b).sortByKey(\"asc\")\n",
    "\n",
    "rdd2.foreach(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dois\n",
      "um\n",
      "três\n",
      "um\n",
      "dois\n",
      "quatro\n",
      "um\n",
      "cinco\n"
     ]
    }
   ],
   "source": [
    "list2 = [\"um\", \"quatro\", \"cinco\"]\n",
    "\n",
    "rdd2 = spark.sparkContext.parallelize(list2)\n",
    "\n",
    "rddUnion = rdd.union(rdd2)\n",
    "\n",
    "rddUnion.foreach(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "um\n"
     ]
    }
   ],
   "source": [
    "rddIntersection = rdd.intersection(rdd2)\n",
    "rddIntersection.foreach(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "um\n",
      "dois\n",
      "três\n"
     ]
    }
   ],
   "source": [
    "rddDistinct = rdd.distinct()\n",
    "rddDistinct.foreach(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Pedro', (38, 'BH'))\n",
      "('Maria', (42, 'DF'))\n"
     ]
    }
   ],
   "source": [
    "list = [(\"Pedro\", 38), (\"Maria\", 42), (\"João\", 12)]\n",
    "list2 = [(\"Pedro\", \"BH\"), (\"Maria\", \"DF\")]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(list)\n",
    "rdd2 = spark.sparkContext.parallelize(list2)\n",
    "\n",
    "rddJoin = rdd.join(rdd2)\n",
    "\n",
    "rddJoin.foreach(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [(\"Pedro\", 38), (\"Maria\", 42), (\"João\", 12)]\n",
    "list2 = [(\"Pedro\", \"BH\"), (\"Maria\", \"DF\")]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(list)\n",
    "rdd2 = spark.sparkContext.parallelize(list2)\n",
    "\n",
    "rddJoin = rdd.join(rdd2)\n",
    "#rddJoin.foreach(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Maria', (42, 'DF')), ('Pedro', (38, 'BH'))]\n"
     ]
    }
   ],
   "source": [
    "print(rddJoin.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(rddJoin.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['um', 'um', 'dois', 'dois']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rddUnion.foreach(print)\n",
    "rddUnion.take(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['um', 'um', 'um', 'três']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rddUnion.foreach(print)\n",
    "rddUnion.top(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'um': 3, 'dois': 2, 'três': 1, 'quatro': 1, 'cinco': 1})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddUnion.countByValue()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'um um dois dois três um quatro cinco'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddUnion.reduce(lambda a,b : a + ' ' + b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saveAsTextFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o414.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/deborabastos/Documents/5.Bootcamp DS/Módulo2/bootcamp_code already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/deborabastos/Documents/5.Bootcamp DS/Módulo2/bootcamp_code/4.spark_transformation.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/deborabastos/Documents/5.Bootcamp%20DS/M%C3%B3dulo2/bootcamp_code/4.spark_transformation.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rddUnion\u001b[39m.\u001b[39;49msaveAsTextFile(\u001b[39m'\u001b[39;49m\u001b[39m/Users/deborabastos/Documents/5.Bootcamp DS/Módulo2/bootcamp_code/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/rdd.py:2205\u001b[0m, in \u001b[0;36mRDD.saveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   2203\u001b[0m     keyed\u001b[39m.\u001b[39m_jrdd\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mBytesToString())\u001b[39m.\u001b[39msaveAsTextFile(path, compressionCodec)\n\u001b[1;32m   2204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2205\u001b[0m     keyed\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mBytesToString())\u001b[39m.\u001b[39;49msaveAsTextFile(path)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o414.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/deborabastos/Documents/5.Bootcamp DS/Módulo2/bootcamp_code already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "rddUnion.saveAsTextFile('/Users/deborabastos/Documents/5.Bootcamp DS/Módulo2/bootcamp_code/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
